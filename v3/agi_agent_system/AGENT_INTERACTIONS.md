## AGI Agent System: Agent Interaction Analysis

This document provides a detailed analysis of the interactions between the Planner, Developer, and Critic agents within the `v3/agi_agent_system`. It covers their prompts, output structures, how they modify the shared `WorkflowState`, and the logic of the workflow orchestration.

---

### 1. Planner Agent (`agents/planner.py`)

The Planner Agent's primary role is to take a high-level user goal and decompose it into a structured list of manageable sub-tasks.

**A. Key Elements of `PLANNER_PROMPT`:**

The `PLANNER_PROMPT` is designed to guide the LLM in generating a coherent and structured task plan.
*   **Core Instruction**: "목표를 하위 태스크로 분해해주세요." (Please break down the goal into sub-tasks.)
*   **Input Variable**:
    *   `{goal}`: This placeholder is filled with the user-defined goal provided to the system.
*   **Output Structure Requirements**: The prompt explicitly defines the expected JSON structure for each task, requiring:
    *   `task_id`: A unique integer identifier for the task.
    *   `description`: A detailed textual description of what the task involves.
    *   `priority`: A numerical priority for the task (1-5, where 5 is the highest).
    *   `dependencies`: A list of `task_id`s that the current task depends on.
*   **Constraints and Formatting Guidance**:
    *   `task_id` should be a 1-based integer.
    *   `dependencies` must be an array of integers (e.g., `[1, 2, 3]`).
    *   Descriptions should be "명확하고 구체적인" (clear and specific).
*   **Format Instructions Placeholder**:
    *   `{format_instructions}`: This is a standard part of LangChain prompts when using Pydantic output parsers. The `BaseAgent` populates this with formatting guidance derived from the `TaskPlan` Pydantic model, helping the LLM generate output that can be correctly parsed into the desired Pydantic objects.

**B. Output Pydantic Models:**

*   **`SubTask(BaseModel)`**: Represents a single task in the plan.
    *   `task_id: int = Field(description="태스크 ID")`
    *   `description: str = Field(description="태스크 설명")`
    *   `priority: int = Field(description="우선순위 (1-5, 5가 가장 높음)")`
    *   `dependencies: List[int] = Field(description="의존하는 태스크 ID 목록 (정수만 사용)")`
        *   The `PlannerAgent` includes a `parse_dependencies` helper function to ensure that values in this list are integers, even if the LLM returns them in a slightly different format (e.g., as strings like "tasks.1").

*   **`TaskPlan(BaseModel)`**: Represents the complete list of tasks.
    *   `tasks: List[SubTask] = Field(description="하위 태스크 목록")`

**C. `run` Method and `WorkflowState` Updates:**

The `PlannerAgent.run(state: Dict[str, Any])` method executes the planning step:
1.  It formats the `PLANNER_PROMPT` using the `goal` from the input `state`.
2.  Invokes the LLM to generate the task plan.
3.  Uses a helper function `extract_json` to reliably get the JSON portion from the LLM's potentially verbose response.
4.  Parses the extracted JSON string into a Python dictionary.
5.  It then calls `parse_dependencies` for each task to ensure dependency IDs are integers.
6.  The dictionary is then validated and converted into a `TaskPlan` Pydantic object.
7.  Each task from `task_plan.tasks` is logged to memory via `self.append_conversation("planner", task.dict())`.
8.  **Key `WorkflowState` modifications**:
    *   `state["tasks"] = task_plan.tasks`: The `tasks` key in the `WorkflowState` is updated with the list of `SubTask` objects generated by the planner.
    *   `state["current_task_index"] = 0`: The `current_task_index` is set to `0`, signaling that the workflow should begin processing from the first task in the newly created plan.

---

### 2. Developer Agent (`agents/developer.py`)

The Developer Agent is responsible for generating code to solve a specific task provided by the Planner.

**A. Key Elements of `DEVELOPER_PROMPT`:**

The `DEVELOPER_PROMPT` instructs the LLM to act as a software developer.
*   **Role Definition**: "당신은 주어진 태스크를 해결하는 코드를 생성하는 개발자입니다." (You are a developer who generates code to solve a given task.)
*   **Input Variables**:
    *   `{task_description}`: This is filled with the `description` field of the current `SubTask` being processed (i.e., `state["tasks"][state["current_task_index"]].description`).
    *   `{previous_results}`: This provides context from tasks that have already been completed. It's constructed by iterating from the first task up to (but not including) the `current_task_index`, and appending their results from `state["results"]`. If no tasks have been completed yet, it defaults to "없음" (None).
*   **Output Structure Requirements**: The prompt specifies the expected JSON structure for the solution:
    *   `code`: The actual source code.
    *   `explanation`: A detailed explanation of the code.
    *   `test_cases`: A list of test cases to verify the code's functionality.
*   **Format Instructions Placeholder**:
    *   `{format_instructions}`: Populated by `BaseAgent` based on the `CodeSolution` Pydantic model.

**B. Output Pydantic Model:**

*   **`CodeSolution(BaseModel)`**: Defines the structure for the developer's output.
    *   `code: str = Field(description="생성된 코드")`
    *   `explanation: str = Field(description="코드에 대한 설명")`
    *   `test_cases: List[str] = Field(description="테스트 케이스 목록")`

**C. `run` Method and `WorkflowState` Updates:**

The `DeveloperAgent.run(state: Dict[str, Any])` method executes the code generation step:
1.  It identifies the `current_task` using `state["tasks"][state["current_task_index"]]`.
2.  It compiles `previous_results` by looking at earlier entries in `state["results"]`.
3.  Formats the `DEVELOPER_PROMPT` with the current task's description and the compiled previous results.
4.  Invokes the LLM to generate the code solution.
5.  Parses the LLM's response into a `CodeSolution` object using `self.output_parser`.
6.  Logs the generated solution to memory: `self.append_conversation("developer", {"task_id": current_task.task_id, "content": solution.dict()})`.
7.  **Key `WorkflowState` modifications**:
    *   `if "results" not in state: state["results"] = []` : Initializes `results` if not present.
    *   `state["results"].append(solution.dict())`: The dictionary representation of the `CodeSolution` object is appended to the `results` list in the `WorkflowState`.
    *   **Note on Retries and Data Access**: When a task is retried, the Developer Agent appends the new result to the `state["results"]` list. The Critic Agent later accesses `state["results"][state["current_task_index"]]`. If `current_task_index` points to a task that has been retried, this indexing would fetch the result from the *first* attempt for that task index, not the latest one, because new results for retries are appended, making the list longer and shifting indices for subsequent attempts if not handled carefully. The `should_continue` logic re-uses the same `current_task_index` for retries. This implies that for the Critic to evaluate the *latest* attempt, the `results` list should ideally store only one result per `current_task_index` (e.g., by overwriting or managing a list of lists). However, describing the current code: the `append` behavior is what's implemented.

---

### 3. Critic Agent (`agents/critic.py`)

The Critic Agent evaluates the code solution produced by the Developer Agent against the task requirements and provides feedback.

**A. Key Elements of `CRITIC_PROMPT`:**

The `CRITIC_PROMPT` sets up the LLM to act as a code reviewer.
*   **Role Definition**: "당신은 생성된 코드를 평가하고 피드백을 제공하는 비평가입니다." (You are a critic who evaluates generated code and provides feedback.)
*   **Input Variables**:
    *   `{task_description}`: The description of the current `SubTask` (from `state["tasks"][state["current_task_index"]].description`).
    *   `{code}`: The code generated by the Developer (from `state["results"][state["current_task_index"]]["code"]`).
    *   `{explanation}`: The explanation provided by the Developer (from `state["results"][state["current_task_index"]]["explanation"]`).
    *   `{test_cases}`: The test cases from the Developer, joined into a multi-line string (from `state["results"][state["current_task_index"]]["test_cases"]`).
    *   `{previous_results}`: Context from previously completed tasks, similar to how the Developer Agent receives it.
    *   `{success_threshold}`: The minimum score (e.g., 0.8) for the task to be considered successfully completed. This value is taken directly from `config.success_threshold`.
*   **Output Structure Requirements**: The prompt specifies the JSON structure for the evaluation:
    *   `score`: A numerical quality score (0-1).
    *   `feedback`: Detailed textual feedback on the code.
    *   `improvements`: A list of specific suggestions for improvement.
    *   `is_success`: A boolean indicating if the `score` meets or exceeds the `success_threshold`.
*   **Format Instructions Placeholder**:
    *   `{format_instructions}`: Populated by `BaseAgent` based on the `CodeEvaluation` Pydantic model.

**B. Output Pydantic Model:**

*   **`CodeEvaluation(BaseModel)`**: Defines the structure for the critic's feedback.
    *   `score: float = Field(description="평가 점수 (0-1)")`
    *   `feedback: str = Field(description="코드에 대한 피드백")`
    *   `improvements: List[str] = Field(description="개선 사항 목록")`
    *   `is_success: bool = Field(description="성공 여부")`

**C. `run` Method and `WorkflowState` Updates:**

The `CriticAgent.run(state: Dict[str, Any])` method executes the evaluation step:
1.  Retrieves the `current_task` (from `state["tasks"]`) and `current_result` (the developer's solution from `state["results"]`) using `state["current_task_index"]`. (See note in Developer section about which result is accessed if retries occur).
2.  Compiles `previous_results`.
3.  Formats the `CRITIC_PROMPT` with all the necessary information, including the `success_threshold` from the global `config`.
4.  Invokes the LLM to generate the evaluation.
5.  Parses the LLM's response into a `CodeEvaluation` object.
6.  Logs the evaluation to memory: `self.append_conversation("critic", {"task_id": current_task.task_id, "content": evaluation.dict()})`.
7.  **Key `WorkflowState` modifications**:
    *   `if "evaluations" not in state: state["evaluations"] = []`: Initializes `evaluations` if not present.
    *   `state["evaluations"].append(evaluation.dict())`: The dictionary representation of the `CodeEvaluation` object is appended to the `evaluations` list in `WorkflowState`.
    *   `state["iterations"] = state.get("iterations", 0) + 1`: The `iterations` counter (specific to the current task's development-critique loop) is incremented.

---

### 4. Workflow Logic (`workflow/agent_graph.py`)

The `agent_graph.py` file defines the overall control flow using LangGraph, with the `should_continue` function being central to the iterative refinement process.

**A. `WorkflowState` (TypedDict):**
This dictionary structure holds the shared state that is passed between nodes in the graph:
*   `goal: str`: The initial user-defined goal.
*   `tasks: List[Any]`: The list of `SubTask` objects generated by the Planner. (Note: Typed as `List[Any]`, but in practice, these are `SubTask` instances or dictionaries representing them).
*   `current_task_index: int`: The index of the task currently being processed from the `tasks` list.
*   `iterations: int`: A counter for the number of times the Developer-Critic loop has been executed for the *current* task.
*   `results: List[Dict[str, Any]]`: A list of `CodeSolution` dictionaries from the Developer.
*   `evaluations: List[Dict[str, Any]]`: A list of `CodeEvaluation` dictionaries from the Critic.

**B. `should_continue(state: WorkflowState) -> str` Function:**

This function is the conditional edge in the LangGraph, called after the Critic Agent completes its run. It determines the next state in the workflow.

1.  **Inputs from `WorkflowState` Used for Decision**:
    *   `current_evaluation = state["evaluations"][-1]`: It retrieves the most recent evaluation from the Critic (the last item in the `evaluations` list). The key piece of information used from this is `current_evaluation["is_success"]`.
    *   `state["iterations"]`: It checks the current number of refinement attempts for the active task against `config.max_iterations`.
    *   `state["current_task_index"]` and `len(state["tasks"])`: Used to determine if all tasks in the plan have been processed when deciding to move to the next task or end the workflow.
    *   `state["tasks"]`: Used to get the `current_task` object via `state["tasks"][state["current_task_index"]]`. While the `current_task` object itself is fetched, its attributes are not directly used in the branching logic of `should_continue` beyond its index being managed.

2.  **Decision Logic and Use of `config.max_iterations`**:
    The core decision logic is as follows:
    ```python
    current_task = state["tasks"][state["current_task_index"]] 
    current_evaluation = state["evaluations"][-1]
    
    # Condition: Successful evaluation OR maximum iterations reached
    if current_evaluation["is_success"] or state["iterations"] >= config.max_iterations:
        # Action: Move to the next task or end workflow
        state["current_task_index"] += 1  # Increment to process the next task
        state["iterations"] = 0          # Reset iteration counter for the new task
        
        # Check if all tasks are completed
        if state["current_task_index"] >= len(state["tasks"]):
            return "end"  # All tasks done, transition to "end" node
        return "developer" # More tasks remain, transition to "developer" for the next task
    else:
        # Condition: Evaluation failed AND iterations limit not reached
        # Action: Retry the current task
        return "developer" # Transition back to "developer" to retry the current task
    ```
    *   **`config.max_iterations`**: This value is intended to be sourced from the global `config` object (instance of `Config` from `core/config.py`). The provided `Config` class definition in `core/config.py` (`model_path`, `temperature`, `max_tokens`, `success_threshold`, `memory_dir`) does not explicitly list `max_iterations` as an attribute. For `agent_graph.py` to use `config.max_iterations` without error, this attribute must be present on the `config` object at runtime (e.g., set by default in `load_config` if `MAX_ITERATIONS` env var exists, or if the `Config` class were extended). Assuming it's available, it limits how many times the Developer-Critic loop runs for one task.
    *   **`current_evaluation["is_success"]`**: This boolean flag, part of the `CodeEvaluation` model from the Critic, indicates if the solution met the `config.success_threshold`.

3.  **Clarification on Possible Return Values of `should_continue`**:
    *   The docstring for `should_continue` in `agent_graph.py` states: `Returns: str: 다음 단계 ("developer", "critic", "end")` which translates to "Next step ('developer', 'critic', 'end')".
    *   However, a thorough review of the `should_continue` function's implementation (as detailed in the code block above) shows that it **only returns `"developer"` or `"end"`**. There are no conditions under which it would return `"critic"`.
    *   The conditional edges in the `run_workflow` function are defined as:
        ```python
        workflow.add_conditional_edges(
            "critic",
            should_continue,
            {
                "developer": "developer", # Maps the string "developer" to the "developer" node
                "end": "end"            # Maps the string "end" to the "end" node
            }
        )
        ```
        This mapping confirms that the graph expects `should_continue` to return either `"developer"` or `"end"`. If it were to return `"critic"`, it would result in a `KeyError` because `"critic"` is not a key in this mapping dictionary. Thus, the docstring seems to list a potential but unimplemented path, while the actual code follows a more direct loop (Critic -> Developer or Critic -> End).

This structured interaction, orchestrated by LangGraph, enables the system to systematically plan tasks, generate solutions, and iteratively refine them based on critical evaluations, operating within defined constraints such as the maximum number of iterations per task.
---
